{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named hep_ml.losses",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1a9b532adcc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhep_ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBinFlatnessLossFunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhep_ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradientboosting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUGradientBoostingClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named hep_ml.losses"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "## version 2 0.983843\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from hep_ml.losses import BinFlatnessLossFunction\n",
    "from hep_ml.gradientboosting import UGradientBoostingClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_training_data():\n",
    "    filter_out = ['id', 'min_ANNmuon', 'production', 'mass', 'signal', 'SPDhits', 'IP', 'IPSig', 'isolationc']\n",
    "    f = open('../data/training.csv')\n",
    "    data = []\n",
    "    y = []\n",
    "    ids = []\n",
    "    for i, l in enumerate(f):\n",
    "        if i == 0:\n",
    "            labels = l.rstrip().split(',')\n",
    "            label_indices = dict((l, i) for i, l in enumerate(labels))\n",
    "            continue\n",
    "\n",
    "        values = l.rstrip().split(',')\n",
    "        filtered = []\n",
    "        for v, l in zip(values, labels):\n",
    "            if l not in filter_out:\n",
    "                filtered.append(float(v))\n",
    "\n",
    "        label = values[label_indices['signal']]\n",
    "        ID = values[0]\n",
    "\n",
    "        data.append(filtered)\n",
    "        y.append(float(label))\n",
    "        ids.append(ID)\n",
    "    return ids, np.array(data), np.array(y)\n",
    "\n",
    "\n",
    "def get_test_data():\n",
    "    filter_out = ['id', 'min_ANNmuon', 'production', 'mass', 'signal', 'SPDhits', 'IP', 'IPSig', 'isolationc']\n",
    "    f = open('../data/test.csv')\n",
    "    data = []\n",
    "    ids = []\n",
    "    for i, l in enumerate(f):\n",
    "        if i == 0:\n",
    "            labels = l.rstrip().split(',')\n",
    "            continue\n",
    "\n",
    "        values = l.rstrip().split(',')\n",
    "        filtered = []\n",
    "        for v, l in zip(values, labels):\n",
    "            if l not in filter_out:\n",
    "                filtered.append(float(v))\n",
    "\n",
    "        ID = values[0]\n",
    "        data.append(filtered)\n",
    "        ids.append(ID)\n",
    "    return ids, np.array(data)\n",
    "\n",
    "\n",
    "def preprocess_data(X, scaler=None):\n",
    "    if not scaler:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    return X, scaler\n",
    "\n",
    "# get training data\n",
    "ids, X, y = get_training_data()\n",
    "print('Data shape:', X.shape)\n",
    "\n",
    "# shuffle the data\n",
    "np.random.seed(671) # used to be 369\n",
    "np.random.shuffle(X)\n",
    "np.random.seed(671)\n",
    "np.random.shuffle(y)\n",
    "\n",
    "print('Signal ratio:', np.sum(y) / y.shape[0])\n",
    "\n",
    "# preprocess the data\n",
    "X, scaler = preprocess_data(X)\n",
    "y = np_utils.to_categorical(y)\n",
    "\n",
    "# split into training / evaluation data\n",
    "nb_train_sample = int(len(y) * 0.78) # used to be 0.97, 0.78 is better, 0.83 possible\n",
    "X_train = X[:nb_train_sample]\n",
    "X_eval = X[nb_train_sample:]\n",
    "y_train = y[:nb_train_sample]\n",
    "y_eval = y[nb_train_sample:]\n",
    "\n",
    "print('Train on:', X_train.shape[0])\n",
    "print('Eval on:', X_eval.shape[0])\n",
    "\n",
    "# deep pyramidal MLP, narrowing with depth\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.13))\n",
    "model.add(Dense(X_train.shape[1], 75))\n",
    "model.add(PReLU((75,)))\n",
    "\n",
    "model.add(Dropout(0.11))\n",
    "model.add(Dense(75, 50))\n",
    "model.add(PReLU((50,)))\n",
    "\n",
    "model.add(Dropout(0.09))\n",
    "model.add(Dense(50, 30))\n",
    "model.add(PReLU((30,)))\n",
    "\n",
    "model.add(Dropout(0.07))\n",
    "model.add(Dense(30, 25))\n",
    "model.add(PReLU((25,)))\n",
    "\n",
    "model.add(Dense(25, 2))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# train model\n",
    "model.fit(X_train, y_train, batch_size=128, nb_epoch=150, validation_data=(X_eval, y_eval), verbose=2, show_accuracy=True)\n",
    "# nb_epoch = 50, 100 fine\n",
    "\n",
    "# generate submission\n",
    "ids, X = get_test_data()\n",
    "print('Data shape:', X.shape)\n",
    "X, scaler = preprocess_data(X, scaler)\n",
    "predskeras = model.predict(X, batch_size=256)[:, 1]\n",
    "\n",
    "print(\"Load the training/test data using pandas\")\n",
    "train = pd.read_csv(\"../data/training.csv\")\n",
    "test  = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "print(\"Eliminate SPDhits, which makes the agreement check fail\")\n",
    "features = list(train.columns[1:-5])\n",
    "print(\"Train a UGradientBoostingClassifier\")\n",
    "loss = BinFlatnessLossFunction(['mass'], n_bins=15, uniform_label=0)\n",
    "clf = UGradientBoostingClassifier(loss=loss, n_estimators=150, subsample=0.1, # n_estimators = 75\n",
    "                                  max_depth=7, min_samples_leaf=10,\n",
    "                                  learning_rate=0.1, train_features=features, random_state=11)\n",
    "clf.fit(train[features + ['mass']], train['signal'])\n",
    "fb_preds = clf.predict_proba(test[features])[:,1]\n",
    "print(\"Train a Random Forest model\")\n",
    "rf = RandomForestClassifier(n_estimators=250, n_jobs=-1, criterion=\"entropy\", random_state=1)\n",
    "rf.fit(train[features], train[\"signal\"]) # used to be n_estimators=300, 375 is better, 250 could be fine\n",
    "\n",
    "print(\"Train a XGBoost model\")\n",
    "params = {\"objective\": \"binary:logistic\",\n",
    "          \"eta\": 0.2,# used to be 0.2 or 0.1\n",
    "          \"max_depth\": 7, # used to be 5 or 6\n",
    "          \"min_child_weight\": 1,\n",
    "          \"silent\": 1,\n",
    "          \"colsample_bytree\": 0.7,\n",
    "          \"seed\": 1}\n",
    "num_trees=450 #used to be 300, 375 is better\n",
    "gbm = xgb.train(params, xgb.DMatrix(train[features], train[\"signal\"]), num_trees)\n",
    "\n",
    "print(\"Make predictions on the test set\")\n",
    "# test_probs = (0.35*rf.predict_proba(test[features])[:,1]) + (0.35*gbm.predict(xgb.DMatrix(test[features])))+(0.15*predskeras) + (0.15*fb_preds) \n",
    "test_probs = (0.24*rf.predict_proba(test[features])[:,1]) + (0.3*gbm.predict(xgb.DMatrix(test[features])))+(0.26*predskeras) + (0.20*fb_preds) #is better\n",
    "# test_probs = (0.25*rf.predict_proba(test[features])[:,1]) + (0.25*gbm.predict(xgb.DMatrix(test[features])))+(0.25*predskeras) + (0.25*fb_preds)\n",
    "submission = pd.DataFrame({\"id\": test[\"id\"], \"prediction\": test_probs})\n",
    "submission.to_csv(\"rf_xgboost_keras_flatness_v5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
